{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifiers\n",
    "\n",
    "A way to improve a set of independent classifiers is to aggregate the prediction of each classsifier and predict the class the gets the most most from all the classifiers. This prediction by majority vote is called a *hard voting* classifier. This classifier made up of multiple classifiers performs better than the best classifier in the ensemble. Furthermore, a *strong learner* (a classifier that has high accuracy) can be constructed by aggregating *weak learner* (classifiers that perform slightly better than guessing) into an ensemble. This performance improvement can be achieved if there are enough weak learners and are sufficiently diverse. \n",
    "\n",
    "The idea behind the improvement achieved with ensembles of classifiers is the law of large numbers. For example, if there is an ensemble that has 1000 classifiers each with an accuracy of 51%, the prediction accuracy using the majority voted class should be around 75%. This happens because the more classifiers there are, the more the ratio of classification gets to 51% instead of fluctuating around 51%. This works as long as the classifiers in the ensemble are independet from each other. That is not always the case, because classifiers tend to make the same type of errors. This is why very different algorithms are used in an ensemble That way, the chance that the algorithms are making different errors is greater, so the accuracy of the ensemble is increased.\n",
    "\n",
    "The code below creates a voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
    "for train_index, test_index in split.split(X, y):\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.966666666667\n",
      "RandomForestClassifier 0.933333333333\n",
      "SVC 0.966666666667\n",
      "VotingClassifier 0.966666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all the classifiers have the method `predict_proba()` to estimate class probabilities, then it is possible to predict the highest class probability using the average from the individual classifiers. This method is called *soft voting* and most of the time performs better than *hard voting* because it gives more weight to votes with high confidence. To use soft voting instead of hard voting, it is enough to set `voting='soft` and make sure all classifiers in the ensemble are capable of estimating probabilities. \n",
    "\n",
    "# Bagging and Pasting\n",
    "\n",
    "An option to get diverse classifiers, different from using different algorithms in the ensemble, is to train a single algorithm using random subsets of the trainig set. When sampling is performed with replacement it is called **bagging**, while **pasting** is when there is sampling without replacement. (For more about with and without replacement visit [Sampling With or Without Replacement](https://www.thoughtco.com/sampling-with-or-without-replacement-3126563))That is, bagging allows training instaces to be sampled several times for the same predictor. \n",
    "\n",
    "Once the data is sampled, the predictots are trained using the subsets and the ensemble makes a prediction aggregating the predictions of all the predictors. The aggregation function generally is the *statistical mode*, or the most frequent prediction. Since the predictor was trained using subsets of the data, it has higher bias than if the whole training set was used. However, the aggregation procedure reduces variance and bias. In general, the ensemble has a lower variance and a similar bias than a single classifier in the ensemble. \n",
    "\n",
    "Since the individual classifiers can be trained and make predictions in parallel, the methods of bagging and pasting scale very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
